{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aed1afd-a44d-4cde-a65a-caa41446f6b3",
   "metadata": {},
   "source": [
    "# AI Inference Test Framework\n",
    "This framework provides a complete environment for testing, interacting with, and building agentic workflows using Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edcef8f-0950-4d56-a839-2fdcbee8d7b9",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The system integrates several components to deliver a flexible and extensible inference stack:\n",
    "\n",
    "- Open WebUI: Chat frontend for interacting with LLMs\n",
    "- n8n: Workflow automation and orchestration\n",
    "- PostgreSQL + pgVector: Vector database for embedding storage and retrieval\n",
    "- trt-llm: Optimized inference server for efficient model execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f62454-0c54-40bd-abd3-4a22037a8470",
   "metadata": {},
   "source": [
    "# Usage\n",
    "The framework is fully operational immediately after deployment.\n",
    "Additional LLMs can be integrated into Open WebUI following the official guide: <br>\n",
    "[üëâ Open WebUI Quick Start Guide](https://docs.openwebui.com/getting-started/quick-start/starting-with-openai)\n",
    "\n",
    "By default, the framework deploys the TinyLlama-1.1B-Chat-v1.0 model.\n",
    "Optionally, you can deploy an NVIDIA NIM for GPU-accelerated inference using the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a519b3-85a8-4df2-ae2f-6a51bbb64617",
   "metadata": {},
   "source": [
    "# (Optional) Deploying a Nvidia NIM\n",
    "Since this is a test environment, only one LLM should be deployed at a time.\n",
    "Before deploying a new NIM model, the old one must be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187ccfb-c131-42b3-9560-fabdf5825d1a",
   "metadata": {},
   "source": [
    "### Step 0 ‚Äî Stop and Remove the Existing Container\n",
    "Check for existing containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9e530-0012-463a-aab8-8fb179f06b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc8c1d-00ff-4295-80cb-8375c8d223bc",
   "metadata": {},
   "source": [
    "Stop and remove the current TRT-LLM container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8102565-7b2c-4488-a307-dae45edac00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker kill trt-llm && docker rm trt-llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d1315c-46f0-405c-ab1a-d2549d158ac9",
   "metadata": {},
   "source": [
    "Check that no LLM container is active anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0a072-d837-4d84-96f8-045dcd26d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805c2a8-4ab8-4ef9-870a-d60ac295d5d7",
   "metadata": {},
   "source": [
    "If trt-llm is not listed anymore, the environment is clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107844e-9733-4abd-bf67-55e28f7e49ed",
   "metadata": {},
   "source": [
    "### Step 1 - Add the API Key and login to container registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b36326-598c-4bad-baee-0431dedc8c44",
   "metadata": {},
   "source": [
    "An NGC API key is required to access NGC resources and a key can be generated here: [NGC Catalog](https://org.ngc.nvidia.com/setup/api-keys).\n",
    "\n",
    "When creating an NGC API key, ensure that at least ‚ÄúNGC Catalog‚Äù is selected from the ‚ÄúServices Included‚Äù dropdown. <br>\n",
    "More Services can be included if this key is to be reused for other purposes.\n",
    "\n",
    "![Image](https://docs.nvidia.com/nim/large-language-models/latest/_images/personal-key.png)\n",
    "\n",
    "2. Add the API Key in the next column:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946a6bc-453a-4c1e-b4bb-4c1584361e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NGC_API_KEY']=\"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\"  #THIS NEEDS TO BE CHANGED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45b92e-fa66-47d8-81c4-783c580dc991",
   "metadata": {},
   "source": [
    "3. Login to the nvcr.io container registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f1aef-174e-4220-9fe1-e39e2d8b62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $NGC_API_KEY | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440734f0-42d4-4b5d-b6c4-e8e2514268e7",
   "metadata": {},
   "source": [
    "### Step 2 - Configure the local NIM cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6a159-c33c-4609-9e3e-a4706181b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LOCAL_NIM_CACHE'] = os.path.expanduser(\"~/.cache/nim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdd6502-73cb-48fb-beb5-6812bfa7f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"$LOCAL_NIM_CACHE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e45de3f-a7fc-4ea7-ad83-48d5166e039c",
   "metadata": {},
   "source": [
    "Ensure that the directory is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca56d1-72f1-480c-8eba-976ad7d9c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lha ~/.cache/ | grep nim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1fdad-3757-4ac4-a43b-34ea4663487d",
   "metadata": {},
   "source": [
    "If the nim folder exists, the setup is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1decd81-3603-44d5-8c0d-a990af4cbe5f",
   "metadata": {},
   "source": [
    "### Step 3 - Deploy NIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214aea52-5820-44c7-9b07-5cafc6b03645",
   "metadata": {},
   "source": [
    "Now the actual NIM will be deployed. This example uses GPT-OSS-20B from the NGC catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe4a55-6f90-490b-8706-b04b8da76a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d --rm --name nvidia-nim --gpus all --shm-size=16GB -e NGC_API_KEY=$NGC_API_KEY -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" -u $(id -u) -p 8000:8000 nvcr.io/nim/openai/gpt-oss-20b:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f8abaa-ef5d-4e03-8466-75f9f2fa68ac",
   "metadata": {},
   "source": [
    "Now that the container is created, we're waiting for it to start up. Execute the next cell to monitor the deployment.\n",
    "\n",
    "Once it's done there will be an output like:\n",
    "> Container is ready! <br>\n",
    "> Proceed with the next steps in the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dadd3c-a4ac-4d67-a3ff-1b101b3c23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "container_name = \"nvidia-nim\"  # Adjust this name to your container\n",
    "search_string = \"Uvicorn running on http://0.0.0.0:8000\"\n",
    "\n",
    "print(f\"Monitoring logs of container '{container_name}'...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        logs = subprocess.check_output(f\"docker logs {container_name}\", shell=True, stderr=subprocess.STDOUT).decode()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error retrieving logs: {e}\")\n",
    "        time.sleep(2)\n",
    "        continue\n",
    "\n",
    "    if search_string in logs:\n",
    "        print(\"Container is ready!\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Not ready yet, waiting 2 seconds...\")\n",
    "        time.sleep(2)\n",
    "\n",
    "print(\"Continue with the next steps in the notebook.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dd84f4-879b-4b32-a490-4e3c200f162b",
   "metadata": {},
   "source": [
    "Once ‚ÄúContainer is ready!‚Äù appears, the model is fully initialized and accessible via Open WebUI.<br>\n",
    "You can now open your Brev instance, connect to the WebUI, and start interacting with the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db388c40-51b4-4c5a-978a-fdaacf58a0cc",
   "metadata": {},
   "source": [
    "### Step 4 - Cleanup\n",
    "\n",
    "If you want to stop the current LLM and deploy a different one, you can terminate the container using the command in the next cell.<br>\n",
    "By modifying the docker run command from Step 3, you can deploy any other LLM available in the NGC catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826d0f9-ff38-4bc8-970e-50b3c4d1de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker kill nvidia-nim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfbdc4-544a-49be-b818-61257a526390",
   "metadata": {},
   "source": [
    "If the nvidia-nim container is no longer listed, the cleanup was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc84f1-67b7-4092-9b6f-e433a0bc75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps -a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
